{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cb106b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import fitz\n",
    "import re\n",
    "import tiktoken\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3577178e",
   "metadata": {},
   "source": [
    "Using with open doesn't work well for pdf so i used pymupdf to parse the pdf and get the text from it. I also removed the first 4845 characters from the text because they were the index and just titles, so we can use the real text from the pdf. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bfd5d80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185038\n",
      "In your schooldays most of you who read this book made acquaintance with the noble\n",
      "building of Euclid's geometry, and you remember — perhaps with more respect than love\n",
      "— the magnificent structure, on\n"
     ]
    }
   ],
   "source": [
    "# Getting file\n",
    "\n",
    "file_path = \"relativity.pdf\"\n",
    "doc = fitz.open(file_path)\n",
    "raw_text = \"\"\n",
    "for page in doc:\n",
    "    raw_text += page.get_text()\n",
    "raw_text = raw_text[4845:]\n",
    "print(len(raw_text))\n",
    "print(raw_text[:200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "52b82c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35454\n",
      "['In', 'your', 'schooldays', 'most', 'of', 'you', 'who', 'read', 'this', 'book', 'made', 'acquaintance', 'with', 'the', 'noble', 'building', 'of', 'Euclid', \"'\", 's', 'geometry', ',', 'and', 'you', 'remember', '—', 'perhaps', 'with', 'more', 'respect', 'than', 'love', '—', 'the', 'magnificent', 'structure', ',', 'on', 'the', 'lofty', 'staircase', 'of', 'which', 'you', 'were', 'chased', 'about', 'for', 'uncounted', 'hours']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(len(preprocessed))\n",
    "print(preprocessed[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f50c1a76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3211"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating the vocabulary\n",
    "all_words = sorted(set(preprocessed))  # using set here so that duplicates are removed.\n",
    "vocab_size = len(all_words)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e0142957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "('+', 5)\n",
      "('+43', 6)\n",
      "(',', 7)\n",
      "('-', 8)\n",
      "('-axis', 9)\n",
      "('-rays', 10)\n"
     ]
    }
   ],
   "source": [
    "vocab = {token:integer for integer, token in enumerate(all_words)}\n",
    "for i, item in enumerate(vocab.items()) :\n",
    "    print(item)\n",
    "    if i >= 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "59a29394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 514, 3150, 7, 3126, 3150, 7, 3198, 0, 1, 202]\n"
     ]
    }
   ],
   "source": [
    "# building our own tokenizer class\n",
    "\n",
    "class Tokenizer1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {integer:item for item, integer in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)                      \n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text\n",
    "    \n",
    "tokenizer = Tokenizer1(vocab)\n",
    "\n",
    "text = \"\"\"\"We will, we will, you!\" By\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb781525",
   "metadata": {},
   "source": [
    "This encoder is only built on the vocab which the class got from the book, so it doesn't yet understand or have any kind of encoding value for words it doesn't know like rock or Queen so i had to remove them from the statement to get a encoded list. We will use byte-pair encoding or <unk> token to incorporate unknown vocab words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9e957a23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" We will, we will, you!\" By'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c546d9b",
   "metadata": {},
   "source": [
    "Now to mitigate the problem we faced above, of unknown words for the vocab. We will use special characters. The two special characters going to be used are : <EndOfText> indicating the end of content from a particular text and <unk> indicating an unknown character which is not found in the defined vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "350e2b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('æther-drift', 3208)\n",
      "('ϖ', 3209)\n",
      "('—', 3210)\n",
      "('<|endoftext|>', 3211)\n",
      "('<|unk|>', 3212)\n"
     ]
    }
   ],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {token:integer for integer, token in enumerate(all_tokens)}\n",
    "\n",
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7727445b",
   "metadata": {},
   "source": [
    "Now let's create a new version of the tokenizer including the two new special tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2336a455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[517, 3198, 3212, 3154, 539, 781, 3212, 53, 1674, 2620, 1888, 53, 3212, 3212, 517, 3198, 2388, 3199, 1632, 1711, 539, 3212, 3212, 7, 53, 3212, 2620, 1888, 53, 1674]\n"
     ]
    }
   ],
   "source": [
    "class Tokenizer2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {v:k for k, v in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([.,;:>!?_\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int\n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "        return text\n",
    "    \n",
    "tokenizer = Tokenizer2(vocab)\n",
    "text1 = \"When you sit with a beautiful girl 2 hours seem like 2 minutes\"\n",
    "text2 = \"When you put your hand in a hot kettle, 2 minutes seem like 2 hours\"\n",
    "    \n",
    "text = \"<|endoftext|>\".join([text1, text2])\n",
    "print(tokenizer.encode(text))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c871e734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When you <|unk|> with a beautiful <|unk|> 2 hours seem like 2 <|unk|> <|unk|> When you put your hand in a <|unk|> <|unk|>, 2 <|unk|> seem like 2 hours\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "befa7465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2215, 345, 1650, 351, 257, 4950, 2576, 362, 2250, 1283, 588, 362, 2431, 50256, 2215, 345, 1234, 534, 1021, 287, 257, 3024, 40231, 11, 362, 2431, 1283, 588, 362, 1711, 325, 11962, 1818, 321, 4070]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "text += \"einsteinworkamazing\"\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1629def7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When you sit with a beautiful girl 2 hours seem like 2 minutes<|endoftext|>When you put your hand in a hot kettle, 2 minutes seem like 2 hourseinsteinworkamazing\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963b4200",
   "metadata": {},
   "source": [
    "The byte pair encoding is the current best encoding method and it is used in models like chatgpt too. After figuring out this tokenizing part, we will figure out data sampling in a sliding window. For this we will use the pytorch's builtin dataset and dataloaders to create an efficient way of retrieving data when we train our LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "85371f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42086\n",
      "x: [818, 534, 5513, 78]\n",
      "y:      [534, 5513, 78, 727]\n"
     ]
    }
   ],
   "source": [
    "encoded_text = tokenizer.encode(raw_text)\n",
    "print(len(encoded_text))\n",
    "context_size = 4\n",
    "\n",
    "x = encoded_text[:context_size]\n",
    "y = encoded_text[1:context_size+1]\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d6b79f",
   "metadata": {},
   "source": [
    "After using the encoder, we will create the dataset class and dataloarders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "70af64ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, text, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        \n",
    "        token_ids = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "        assert len(token_ids) > max_length, \"Number of tokenized inputs must at least be equal to max_length+1\"\n",
    "        for i in range(1, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i: max_length+i]\n",
    "            target_chunk = token_ids[i+1: i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.input_ids[index], self.target_ids[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd5e187",
   "metadata": {},
   "source": [
    "Here, max_length is the size of each input sample the dataset will provide, for example, a max_length of 5 would give an input chunk like [1,2,3,4,5]. The stride is defined as how many tokens the window will shift, if stride is max_length then there would be no overlapping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "724c1b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 534, 5513,   78,  727,  592,  749,  286,  345]]), tensor([[5513,   78,  727,  592,  749,  286,  345,  508]])]\n"
     ]
    }
   ],
   "source": [
    "def create_dataloader_v1(text, batch_size = 4, max_length = 256, stride = 128, shuffle = True, drop_last = True, num_workers = 0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(text, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(dataset, shuffle=shuffle, batch_size=batch_size, drop_last=drop_last, num_workers=num_workers)\n",
    "    return dataloader\n",
    "\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=1, max_length=8, stride=2, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "512e25a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  78,  727,  592,  749,  286,  345,  508, 1100]]), tensor([[ 727,  592,  749,  286,  345,  508, 1100,  428]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8409678d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 592,  749,  286,  345,  508, 1100,  428, 1492]])\n",
      "tensor([[ 749,  286,  345,  508, 1100,  428, 1492,  925]])\n"
     ]
    }
   ],
   "source": [
    "inputs, targets = next(data_iter)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784b39b9",
   "metadata": {},
   "source": [
    "The next thing we are going to create is the embedding matrix. The embedding matrix is a matrix of shape of the dimensions of the vector we want to create for the embedding and the number of unique tokens in the vocabulary. Initially all the weights in the embedding matrix are randomized, then while training the model, these weights are finetuned using backpropogation to minimize the loss function. So basically in each run, it sees that by modifying which weights in in direction and magnitude, is the loss function decreasing by using the gradient descent method. The token vectors we created above don't enclose any kind of semantic meaning, to add semantic meaning to them we create these vectors which initially face in random directions in a high dimensional space but with training they find particular directions where they show some semantic meaning. So for each token id, a embedding vector is created initially for each token in the input. For each token ID, the embedding vector is fixed and shared across all occurrences, regardless of context. However, as the input embedding passes through the model’s layers, it is transformed based on the surrounding words, allowing the model to represent different meanings in different contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a292ea93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  534,  5513,    78,   727],\n",
      "        [  592,   749,   286,   345],\n",
      "        [  508,  1100,   428,  1492],\n",
      "        [  925, 35552,   351,   262],\n",
      "        [15581,   198, 16894,   286],\n",
      "        [48862,   312,   338, 22939],\n",
      "        [   11,   290,   345,  3505],\n",
      "        [  851,  3737,   351,   517]])\n",
      "tensor([[ 5513,    78,   727,   592],\n",
      "        [  749,   286,   345,   508],\n",
      "        [ 1100,   428,  1492,   925],\n",
      "        [35552,   351,   262, 15581],\n",
      "        [  198, 16894,   286, 48862],\n",
      "        [  312,   338, 22939,    11],\n",
      "        [  290,   345,  3505,   851],\n",
      "        [ 3737,   351,   517,  2461]])\n"
     ]
    }
   ],
   "source": [
    "# creating the embedding matrix\n",
    "\n",
    "output_dimension = 256\n",
    "vocab_size = 50257\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dimension)\n",
    "max_length = 4 \n",
    "dataloader_v2 = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=max_length, stride = max_length, shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader_v2)\n",
    "inputs, targets = next(data_iter)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4ca18fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9ef20eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.7396, -0.5382, -0.4037,  ...,  0.7914,  1.2251, -0.8515],\n",
      "        [-0.3755,  0.0399, -0.9273,  ..., -0.3964,  0.1143, -0.0184],\n",
      "        [-1.0424, -0.3457,  0.3157,  ...,  0.1365,  0.4241, -0.6689],\n",
      "        [ 1.1800, -1.1487, -0.6358,  ..., -1.1628,  0.8155,  0.1252]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dimension)\n",
    "\n",
    "print(pos_embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fa7690cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n",
      "tensor([[ 1.7396, -0.5382, -0.4037,  ...,  0.7914,  1.2251, -0.8515],\n",
      "        [-0.3755,  0.0399, -0.9273,  ..., -0.3964,  0.1143, -0.0184],\n",
      "        [-1.0424, -0.3457,  0.3157,  ...,  0.1365,  0.4241, -0.6689],\n",
      "        [ 1.1800, -1.1487, -0.6358,  ..., -1.1628,  0.8155,  0.1252]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "print(pos_embeddings.shape)\n",
    "print(pos_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dc2f2e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dl-env)",
   "language": "python",
   "name": "dl-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
