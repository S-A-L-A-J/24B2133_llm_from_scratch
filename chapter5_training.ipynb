{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f179d9b",
   "metadata": {},
   "source": [
    "In the last chapter, we build the infrastructure to generate text using the complete GPT model. However, the text generated by the model was giberish, i.e. had no meaning or relation to the given text. This was because the model was not training, the weights in the model were completely random. So in this section we will train the model on a smaller set of text. I will try to train my model using more and more text so that it is better but only till the point where my GPU supports.\n",
    "\n",
    "\n",
    "At the beginning of the chapter, the same thing is repeated as the last chapter, which is generated random text from the given input, I am skiping that part and moving on to the calculation of loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6a8bfd",
   "metadata": {},
   "source": [
    "Calculation of loss is a very crucial part of the training process, we try to minimize this loss while training the model to make it more accurate. To do this we use backpropogation, which can be intuitively understood as trying to move the model in the opposite direction of the greatest descent. So, at any given point we calculate the gradient of the loss function and check that towards which direction in the vector space, we find the highest ascent. The direction opposite to that it taken and the model parameters are trained as such. Therefore, the loss function of any model is very important, we studied two types of loss function in this chapter. Cross-entropy and Perplexitiy, Cross entropy loss basically compares the true values and the predicted values and identifies how far they are from each other. Mathematically, it takes the negative of logarithm of the values and takes their average. Perplexitiy is just the exponent of the final loss calculated by cross entropy, so it gives a higher value and it indicates among how many number of tokens is the model unsure which could be the correct one.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7477956",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import fitz\n",
    "import torch.nn as nn\n",
    "import tiktoken\n",
    "from important_GPT_blocks import simple_text_gen\n",
    "from important_GPT_blocks import GPTModel\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 256, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7305f6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[3844,  314,  423, 1716]]), tensor([[3152, 1049, 1176, 2058]])]\n",
      "[tensor([[ 314,  423, 1716, 1918]]), tensor([[1049, 1176, 2058, 1049]])]\n"
     ]
    }
   ],
   "source": [
    "statement_1 = \"Now I have become\"\n",
    "statement_2 = \"With great power comes\"\n",
    "\n",
    "inputs = [text_to_token_ids(\"Now I have become\", tokenizer), text_to_token_ids(\"With great power comes\", tokenizer)]\n",
    "print(inputs)\n",
    "\n",
    "targets = [text_to_token_ids(\" I have become death\", tokenizer), text_to_token_ids(\" great power comes great\", tokenizer)]\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d0098d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[3844,  314,  423, 1716],\n",
    "                      [3152, 1049, 1176, 2058]])\n",
    "\n",
    "targets = torch.tensor([[314,  423, 1716, 1918],\n",
    "                      [1049, 1176, 2058, 1049]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576eaf67",
   "metadata": {},
   "source": [
    "One thing i noticed here, for the targets, if i didn't keep a space before the first character, then the token id for that token is different than the one with the space. This means that the tokenizer gives a specail place to those tokens at the start of a sentence or a sequence. In order to maintain the same token ids I had to provide a space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f07feaf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 50257])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "\n",
    "probas = torch.softmax(logits, dim=-1)\n",
    "probas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed63180b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[34713],\n",
       "         [37311],\n",
       "         [50225],\n",
       "         [21805]],\n",
       "\n",
       "        [[19389],\n",
       "         [21853],\n",
       "         [ 8056],\n",
       "         [29179]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1a6657f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28507f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I have become death\n",
      "\":\"\",\" mystic reclaimed Pew\n"
     ]
    }
   ],
   "source": [
    "print(token_ids_to_text(targets[0], tokenizer))\n",
    "print(token_ids_to_text(token_ids[0].flatten(), tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffdad3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.2187e-05, 2.7385e-05, 3.0733e-05, 1.0088e-05])\n",
      "tensor([1.9483e-05, 2.1743e-05, 1.2926e-05, 1.4988e-05])\n"
     ]
    }
   ],
   "source": [
    "text_ids = 0\n",
    "target_probas_1 = probas[text_ids, [0,1,2,3], targets[text_ids]]\n",
    "print(target_probas_1)\n",
    "\n",
    "text_ids = 1\n",
    "target_probas_2 = probas[text_ids, [0,1,2,3], targets[text_ids]]\n",
    "print(target_probas_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f4e113",
   "metadata": {},
   "source": [
    "It picks the predicted probability assigned to the correct class at each time step 0, 1, 2, and 3 in sequence 0.\n",
    "\n",
    "Let’s break the indexing:\n",
    "\n",
    "probas[0, [0, 1, 2, 3], targets[0]]\n",
    "probas[0] → gets the (T, C) tensor for the first sequence.\n",
    "\n",
    "targets[0] → shape: (T,), gives true class indices for each time step in sequence 0.\n",
    "\n",
    "Now, you're doing advanced indexing:\n",
    "probas[0, t, targets[0][t]] for t ∈{0,1,2,3}\n",
    "So this grabs:\n",
    "\n",
    "[\n",
    "  probas[0, 0, targets[0, 0]],\n",
    "  probas[0, 1, targets[0, 1]],\n",
    "  probas[0, 2, targets[0, 2]],\n",
    "  probas[0, 3, targets[0, 3]]\n",
    "]\n",
    "That is: the predicted probability of the correct class at each of the first 4 time steps of sequence 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f71cd56a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-10.7160, -10.5055, -10.3902, -11.5042, -10.8460, -10.7362, -11.2562,\n",
       "        -11.1083])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "log_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "728c9af2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.8828)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_log_probas = torch.mean(log_probas)\n",
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "neg_avg_log_probas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f159d296",
   "metadata": {},
   "source": [
    "This was the manual implementation of the cross entropy loss, now we will use the inbuilt function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2642df02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.8828)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0,1)\n",
    "targets_flat = targets.flatten()\n",
    "\n",
    "loss = nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44210bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(53253.)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "perplexity.round()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f629b9c3",
   "metadata": {},
   "source": [
    "This means that the model is unsure between 62115, but total number of tokens are 50278 something, i think the extra ones are byte pair encodings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9906b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = \"\"\n",
    "\n",
    "file_path_1= \"relativity.pdf\"\n",
    "doc1 = fitz.open(file_path_1)\n",
    "for page in doc1:\n",
    "    text_data += page.get_text()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2c37ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189883\n",
      "43284\n"
     ]
    }
   ],
   "source": [
    "total_chars = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "print(total_chars)\n",
    "print(total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbfe2b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from important_GPT_blocks import create_dataloader_v1\n",
    "\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * total_chars)\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(train_data,\n",
    "                                    batch_size=2,\n",
    "                                    max_length=GPT_CONFIG_124M['context_length'],\n",
    "                                    stride=GPT_CONFIG_124M['context_length'],\n",
    "                                    drop_last=True,\n",
    "                                    shuffle=True,\n",
    "                                    num_workers=0\n",
    "                                    )\n",
    "\n",
    "val_loader = create_dataloader_v1(val_data,\n",
    "                                    batch_size=2\n",
    "                                    ,\n",
    "                                    max_length=GPT_CONFIG_124M['context_length'],\n",
    "                                    stride=GPT_CONFIG_124M['context_length'],\n",
    "                                    drop_last=False,\n",
    "                                    shuffle=False,\n",
    "                                    num_workers=0\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc06d77",
   "metadata": {},
   "source": [
    "Here we have separated the given data into data loaders fro training and validation. Due to the 90% split for training data, the val loader will have lesser number of batches. Now we will calculate the cross entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "287cc400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = nn.functional.cross_entropy(logits.flatten(0,1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "def loss_batch_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0\n",
    "    if len(data_loader) == 0:\n",
    "        return float('nan')\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    \n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else : \n",
    "            break\n",
    "    return total_loss/num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce4f3e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()  # optional, cleans up interprocess memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84e2b93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.979639454891807\n",
      "10.975638151168823\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_loss = loss_batch_loader(train_loader, model, device)\n",
    "    val_loss = loss_batch_loader(val_loader, model, device)\n",
    "\n",
    "print(train_loss)\n",
    "print(val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be28a86c",
   "metadata": {},
   "source": [
    "So now we have a function ready that can calculate the loss. Now we will start creating the training loop and train our model on data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40950a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter, start_context, tokenizer):\n",
    "    train_losses, val_losses, track_seen_tokens = [], [], []\n",
    "    tokens_count, global_step = 0, -1\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tokens_count += input_batch.numel()\n",
    "            global_step += 1\n",
    "            \n",
    "        if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_seen_tokens.append(tokens_count)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_seen_tokens\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = loss_batch_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = loss_batch_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = simple_text_gen(\n",
    "            model=model, idx=encoded,\n",
    "            max_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \")) \n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6dd89255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000075): Train loss 5.460, Val loss 6.153\n",
      "With great power comes great, and the                                               \n",
      "With great power comes great                                                  \n",
      "With great power comes great is not in the Part II: The Special Theory of Relativity Albert Einstein Albert Einstein Albert Einstein Albert Einstein Albert Einstein Albert Einstein Albert Einstein Albert Einstein Albert Einstein Albert Einstein Albert Einstein Albert Einstein\n",
      "With great power comes great and for the same time of the same way, the reference-rodynamical behaviour of the same                            \n",
      "With great power comes great general law of the (Supplementary to be in the         The theory of relativity,             Part II: Relativity: The General Theory of Rel\n",
      "Ep 6 (Step 000455): Train loss 1.019, Val loss 6.079\n",
      "With great power comes great accuracy. In this connection the the theory of light in vacuo has to the embankment. the reference to be travelling uniformly in the train. But the train. theory of the train. railway-body K,\n",
      "With great power comes great In the other hand the gravitational field. Ought we to smile at the same ray of the discussion in the following law: difficulties of the for the which last is the validity of the \n",
      "With great power comes great In order to attain the greatest possible clearness, let us return to our example of the railway carriage supposed to be travelling uniformly. We call its motion a uniform translation (\"uniform\" because it is of constant velocity and direction,\n",
      "With great power comes great its correctness. The stellar universe ought to be a finite island in the the answer rather differently for the following reason. As a result of the more careful study of electromagnetic phenomena, in the sky when, but without the light emitted by the \n",
      "With great power comes great spectral lines of the motion. On the other hand, the measaring-rod will not experience a shortening in length, as judged from K, if it is applied to the disc in the disc, if we see that a \n",
      "Ep 11 (Step 000835): Train loss 0.137, Val loss 7.125\n",
      "With great power comes great spectral lines in the form of a series, we obtain   When  is small compared with unity, the third of these terms is always small in comparison with the second, which last is alone considered in classical mechanics\n",
      "With great power comes great likewise all equivalent. It possesses a finite volume, which is determined by its \"radius\" (2ϖ2R3). Is it possible to imagine a spherical space? To imagine a space means nothing else than that we imagine\n",
      "With great power comes great its entirety) vanishes at our stand on the ground of classical mechanics, we can satisfy this requirement for our illustration in the following manner. We imagine two clocks of identical construction ; the man at the railway-carriage window\n",
      "With great power comes great spectral lines of the light transmitted to us from a fixed star, as compared with the position of the same spectral lines when they are produced by a terrestrial source of light (Doppler principle). The experimental arguments in favour of the\n",
      "With great power comes great distances the sense of general laws of nature as they are obtained from experience, by means of (c) We speak of the height of the cloud even when the embankment as a standard measure. But we feel the distance w in the\n",
      "Ep 16 (Step 001215): Train loss 0.040, Val loss 7.432\n",
      "With great power comes great distances the sense point diverge farther in the aims of the preceding section. If instead of proud certainty would Of course the general principle of relativity at theend of Section 18. It is certainly true that the observer in the railway carriage experiences\n",
      "With great power comes great distances, and finally, the light plays the part of the Part I: The Special Theory of Relativity Albert Einstein 39   man walking along the carriage, or of the moving point in the present section. If we denote \n",
      "With great power comes great distances the rods is almost as it ought to be according to the rules of Euclidean geometry. Hence the imperfections of the construction of squares in the previous section do not show themselves clearly until this construction is extended over a\n",
      "With great power comes great solid bodies. In every such framework we imagine three surfaces perpendicular to the other marked out, and designated as \" co- ordinate planes \" (\" co-ordinate system \"). A co-ordinate system K then corresponds to the \n",
      "With great power comes great accuracy which has been confirmed with great precision in recent years. The theory of relativity leads to the same law of motion, without requiring any special hypothesis whatsoever as to the structure and the behaviour of the electron. We arrived at a\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 20\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"With great power comes great\", tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bd3fc1",
   "metadata": {},
   "source": [
    "The model has trained quite good, the training loss has become significantly low, however the val loss is still pretty high. I am directly using the code given in the book to visualise the losses in a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e621dcd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  \n",
    "    \n",
    "    ax2 = ax1.twiny() \n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0) \n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045a0898",
   "metadata": {},
   "source": [
    "I tried training the model on more data, i went upto 1.8 lakh tokens but that took way too much time. So, i reduced it and finally settled at 70k tokens. The training loss was very low. Now trying to generate statements using the trained model and then introducting temperature scaling and top-k sampling to make the model even better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8b15679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With great power comes as a crucial test in favour of\n",
      "the theory of relativity, for the electrodynamics of Maxwell-Lorent\n"
     ]
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "token_ids = simple_text_gen( model=model,\n",
    "                            idx=text_to_token_ids(\"With great power comes\", tokenizer),\n",
    "                            max_tokens=25,\n",
    "                            context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117acb8b",
   "metadata": {},
   "source": [
    "Temperature scaling is a method we use to introduce randomness and give variability to the model so that it does not completely stick to the data it has been trained on. The way it works is, for a temperature scale of 1 it directly uses the logit values but for a temperature of lower than 1, it highly scales the ones which are larger closer to 100%, i.e. reduces the value of low value logits and increases the value of high value(kinda like the govt). Whereas, a temperature higher than 1 would display a spread of probabilities into all the logits. This is useful to control how much the model tries to include different tokens. So, if we want the model to experiment and give different results than a higher temperature is used and if we want the model to stick to the exact data being shared then a lower temperature is used. So for medical or educational, or crucial information related tasks the temperature should be kept low, but for creative purposes like writing, story making etc the temperature should be high"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67355640",
   "metadata": {},
   "source": [
    "Top-k sampling is another method where we decide a certain value k, that is how many tokens we are selecting that we want the model to choose between, so let's say we choose k to be 4, then the top 4 highest values are kept as it is, the rest are set to -inf. So, when we apply softmax, the top 4 values sum up to 1, the rest are equal to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30b58d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "    for _ in range(max_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "        if top_k is not None:\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "            probs = torch.softmax(logits, dim=-1)  \n",
    "            idx_next = torch.multinomial(probs, num_samples=1) \n",
    "\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True) \n",
    "            \n",
    "        if idx_next == eos_id:  \n",
    "            break\n",
    "        idx = torch.cat((idx, idx_next), dim=1) \n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01f320f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With great power comes as a crucial test in favour of\n",
      "the theory of relativity, for the electrodynamics of Maxwell-Lorent\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"With great power comes\", tokenizer),\n",
    "    max_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "067b7a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there that the law of a piece of motion ? This form whose plays every reference-\n",
      "other marked out\n"
     ]
    }
   ],
   "source": [
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Once upon a time, there\", tokenizer),\n",
    "    max_tokens=20,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=20,\n",
    "    temperature=2.1\n",
    ")\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dec942",
   "metadata": {},
   "source": [
    "Training the model on a phyiscs book might not have been the best idea. It is not able to talk anything except for physics. :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4fc12125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort takes place with the\n",
      "Lorentz transformation, the law of the transmission of light in vacuo is satisfied both for the\n"
     ]
    }
   ],
   "source": [
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort takes\", tokenizer),\n",
    "    max_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc04187",
   "metadata": {},
   "source": [
    "I think i will train this again on a different book based on a general topic, so that the english becomes better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa2c0a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000195): Train loss 5.886, Val loss 7.281\n",
      "With great power comes great The habits. The habits is a The habits. The habits is a The habits’s The habits. The habits’s the The habits’s The habits. The habits. \n",
      "With great power comes great, but you’re a you’re a you can be you can be a you can be a you’re a you are a you can be a you can be a you can\n",
      "With great power comes great habits. The more likely to the The goal is the The goal is the The type of the The goal is the type of the type of the type of the type of the type of the The goal is the type of\n",
      "With great power comes great The 1st Law: Make it is the The 1st Law: Make it is not only The 1st Law: Make it is the The 1st Law: Make it is an action is an opportunity, and The 1\n",
      "With great power comes great way to be to bundling is to be more the cue. The more likely to be more likely to be more likely to be a habit is a habit is a habit is a habit stacking is to be a habit stacking is a habit\n",
      "Ep 6 (Step 001175): Train loss 2.966, Val loss 6.846\n",
      "With great power comes great and-term habit is a reward is a reward is a reward. and a reward, it’s ’s a reward is ’s, it’s a reward, it’s the �\n",
      "With great power comes great and-term person because they do it.’s the The human behavior change the of levels in the way it. The in profit of The in the number of the cues of the same way that you’s\n",
      "With great power comes great book. Habits are the compound can use to the gym, 80, you want to be a habit, you want to and yourself to you become a Big Difference you do it The more and\n",
      "With great power comes great book, the Tour de France. The next year, the incorrect “It” becomes “The idea leads to prepare I had to apply broadly to all of a after customers had gone being discussed, and I had been a\n",
      "With great power comes great book, I felt alive I will set, I’m not possible to wait to rely even if I don’m indebted to be healthy. curiosity, I had trouble months or college is that are many different forms,\n",
      "Ep 11 (Step 002155): Train loss 0.359, Val loss 7.956\n",
      "With great power comes great mouths of the dopamine-depleted rats. Their little rat faces lit up with pleasurable grins from the tasty substance. Even though dopamine was blocked, they liked the sugar just as much as before; they just didn�\n",
      "With great power comes great book, the most effective way to do. In the the cue can do this new habit into remarkable results if you do I’m not a habit? that happened today. It is to do it? The more take\n",
      "With great power comes great book is actually predictive. All day long, you are making your best guess of how to act given what you’ve just seen and what has worked for you in the past. You are endlessly predicting what will happen\n",
      "With great power comes great book, you must first become the book. If you’re not the way you’re thinking. can use in the system. This finding being a signal that means a diet is one way to keep you to\n",
      "With great power comes great book, you must first become the book. You’re perpetually putting more weight, you’re We are limited. As Robert Plomin, the behavior, the signal that is that we change is the long run, the problem\n",
      "Ep 16 (Step 003135): Train loss 0.057, Val loss 8.507\n",
      "With great power comes great book, you must first become a degree that you to use to do the opposite? You’re all so lazy, you care deeply about what others who 1. areaining an over 100 pounds by asking\n",
      "With great power comes great book, you tend to not because of it becomes every field, but not because we can predispose, but they when it experience one will to form a new habit. ourselves, for many different forms,\n",
      "With great power comes great book, you must first become the book.  Stories like the influence of a better mood, “thinking.” Psychologists refer to this as System 1 (feelings and rapid judgments) versus System 2 (rational\n",
      "With great power comes great book, you must first become the book. You’ll do the right thing in the future by making small change can build better habits is relative. What is the ability to build better habits wither, we inevitable and write jokes.\n",
      "With great power comes great book, which reminds you do it with something you find the methods you rather What makes me lose track of your hands could make a basketball but so on the ball: of his Tiny Habits Method at 5 TO MAKE YOUR HAB\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With great power comesal team as those you tend to be filled out every system is educated and working.\n",
      "How an action has made. In\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "new_text_data = \"\"\n",
    "\n",
    "file_path_1= \"atomic.pdf\"\n",
    "new_doc = fitz.open(file_path_1)\n",
    "for page in new_doc:\n",
    "    new_text_data += page.get_text()\n",
    "\n",
    "new_chars = len(new_text_data)\n",
    "splt = int(train_ratio * new_chars)\n",
    "new_train_data = new_text_data[:splt]\n",
    "new_val_data = new_text_data[splt:]\n",
    "\n",
    "new_train_loader = create_dataloader_v1(new_train_data,\n",
    "                                    batch_size=2,\n",
    "                                    max_length=GPT_CONFIG_124M['context_length'],\n",
    "                                    stride=GPT_CONFIG_124M['context_length'],\n",
    "                                    drop_last=True,\n",
    "                                    shuffle=True,\n",
    "                                    num_workers=0\n",
    "                                    )\n",
    "\n",
    "new_val_loader = create_dataloader_v1(new_val_data,\n",
    "                                    batch_size=2,\n",
    "                                    max_length=GPT_CONFIG_124M['context_length'],\n",
    "                                    stride=GPT_CONFIG_124M['context_length'],\n",
    "                                    drop_last=False,\n",
    "                                    shuffle=False,\n",
    "                                    num_workers=0\n",
    "                                    )\n",
    "\n",
    "new_model = GPTModel(GPT_CONFIG_124M)\n",
    "new_model.to(device)\n",
    "optimizer = torch.optim.AdamW(new_model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 20\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    new_model, new_train_loader, new_val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"With great power comes great\", tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=new_model,\n",
    "    idx=text_to_token_ids(\"With great power comes\", tokenizer).to(device),\n",
    "    max_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da0dc5f",
   "metadata": {},
   "source": [
    "Now let's see how the new model reacts to the old prompt statments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88353fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort takes this list percenton, wheny and make there\n",
      "2017, where I give his wife by writing about in the Monday and\n"
     ]
    }
   ],
   "source": [
    "token_ids = generate(\n",
    "    model=new_model,\n",
    "    idx=text_to_token_ids(\"Every effort takes\", tokenizer).to(device),\n",
    "    max_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cd1a8278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there\n",
      "oficallyonia credit.* over time and back on At the second? And when there are part\n"
     ]
    }
   ],
   "source": [
    "token_ids = generate(\n",
    "    model=new_model,\n",
    "    idx=text_to_token_ids(\"Once upon a time, there\", tokenizer).to(device),\n",
    "    max_tokens=20,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=20,\n",
    "    temperature=2.1\n",
    ")\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7771c6a7",
   "metadata": {},
   "source": [
    "Yeah so now i have two models, one talks about space and physics and one gives life lessons. I understood why we need to train so much so that it can become general stream. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3cf60a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"physics_model.pth\")\n",
    "torch.save(new_model.state_dict(), \"atomic_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05b723e",
   "metadata": {},
   "source": [
    "I tried downloading the weights using the tf method but it threw error and issues, so i will use the alternate method using pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "213a314a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version : 2.5.1\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import tiktoken\n",
    "\n",
    "pkgs = [\"torch\"]\n",
    "for p in pkgs: \n",
    "    print(f\"{p} version : {version(p)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f57c6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"drop_rate\": 0.0,       # Dropout rate\n",
    "    \"qkv_bias\": True        # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-large (774M)\"\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "838f8ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"gpt2-large-774M.pth\"\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "url = f\"https://huggingface.co/rasbt/gpt2-from-scratch-pytorch/resolve/main/{file_name}\"\n",
    "\n",
    "if not os.path.exists(file_name):\n",
    "    urllib.request.urlretrieve(url, file_name)\n",
    "    print(f\"Downloaded to {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "390f046c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "    for _ in range(max_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "        if top_k is not None:\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "            probs = torch.softmax(logits, dim=-1)  \n",
    "            idx_next = torch.multinomial(probs, num_samples=1) \n",
    "\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True) \n",
    "            \n",
    "        if idx_next == eos_id:  \n",
    "            break\n",
    "        idx = torch.cat((idx, idx_next), dim=1) \n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd862fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 1280)\n",
       "  (pos_emb): Embedding(1024, 1280)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (12): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (13): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (14): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (15): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (16): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (17): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (18): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (19): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (20): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (21): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (22): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (23): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (24): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (25): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (26): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (27): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (28): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (29): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (30): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (31): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (32): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (33): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (34): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (35): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=1280, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from important_GPT_blocks import GPTModel\n",
    "gpt = GPTModel(BASE_CONFIG)\n",
    "gpt.load_state_dict(torch.load(file_name, weights_only=True))\n",
    "gpt.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gpt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "115246f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " With great power comes great responsibility.\n",
      "\n",
      "\"I think it's a great opportunity for me. I'm really looking forward to it. I'm really looking forward\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "from add_training_data import text_to_token_ids, token_ids_to_text\n",
    "\n",
    "token_ids = generate(\n",
    "    model=gpt.to(device),\n",
    "    idx=text_to_token_ids(\"With great power\", tokenizer).to(device),\n",
    "    max_tokens=30,\n",
    "    context_size=BASE_CONFIG[\"context_length\"],\n",
    "    top_k=1,\n",
    "    temperature=1.0\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8a182e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Now I have become deathly ill, and the Lord has given me the iniquity of the law, and I am a man, and not the man I was,\n"
     ]
    }
   ],
   "source": [
    "token_ids = generate(\n",
    "    model=gpt.to(device),\n",
    "    idx=text_to_token_ids(\"Now I have become death\", tokenizer).to(device),\n",
    "    max_tokens=30,\n",
    "    context_size=BASE_CONFIG[\"context_length\"],\n",
    "    top_k=1,\n",
    "    temperature=1.0\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99fb040a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Once upon a time, there was a little boy named percy, who was a very good and good-for-nothing man. He was a good man, and a good man's son, and a good\n"
     ]
    }
   ],
   "source": [
    "token_ids = generate(\n",
    "    model=gpt.to(device),\n",
    "    idx=text_to_token_ids(\"Once upon a time, there was a little boy named percy\", tokenizer).to(device),\n",
    "    max_tokens=30,\n",
    "    context_size=BASE_CONFIG[\"context_length\"],\n",
    "    top_k=1,\n",
    "    temperature=1.0\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dl-env)",
   "language": "python",
   "name": "dl-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
