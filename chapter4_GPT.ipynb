{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13f4e517",
   "metadata": {},
   "source": [
    "After completing the attention heads and combining attention heads to form multi-headed attention mechanism. Now, we will form the surrounding tranformer architecture to support the GPT model. So in the book, a dummy model is created first without containing any functionality in it, just to explain how the structre of a GPT model works. First the input is tokenized, them embedded, then runs through multiple attention head mechanisms, linear output layers and then decoded back to tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b4fc680f",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a833d1",
   "metadata": {},
   "source": [
    "\"vocab_size\" indicates a vocabulary size of 50,257 words, supported by the BPE tokenizer discussed in Chapter 2\n",
    "\"context_length\" represents the model's maximum input token count, as enabled by positional embeddings covered in Chapter 2\n",
    "\"emb_dim\" is the embedding size for token inputs, converting each input token into a 768-dimensional vector\n",
    "\"n_heads\" is the number of attention heads in the multi-head attention mechanism implemented in Chapter 3\n",
    "\"n_layers\" is the number of transformer blocks within the model, which we'll implement in upcoming sections\n",
    "\"drop_rate\" is the dropout mechanism's intensity, discussed in Chapter 3; 0.1 means dropping 10% of hidden units during training to mitigate overfitting\n",
    "\"qkv_bias\" decides if the Linear layers in the multi-head attention mechanism (from Chapter 3) should include a bias vector when computing query (Q), key (K), and value (V) tensors; we'll disable this option, which is standard practice in modern LLMs;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bdd87716",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "db677c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.position_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.transformer_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlocks(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias = False\n",
    "        )\n",
    "        \n",
    "    def forward(self, index):\n",
    "        batch_size, seq_len = index.shape\n",
    "        token_embeds = self.token_emb(index)\n",
    "        position_embeds = self.position_emb(torch.arange(seq_len, device = index.device))\n",
    "        x = token_embeds + position_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.transformer_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "    \n",
    "class DummyTransformerBlocks(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps = 1e-5):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa72c85",
   "metadata": {},
   "source": [
    "THe above cell contains the skeleton of a Dummy GPT model, this is how it would look, the classes of transformer blocks and normalization layers are not complete yet, those will be formed step by step by learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ef28e379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[14662,   318,   257],\n",
      "        [33770,  1975,   287]])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "batch = []\n",
    "\n",
    "text1 = \"Life is a\"\n",
    "text2 = \"always believe in\"\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(text1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(text2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "57e82ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 3, 50257])\n",
      "tensor([[[-0.4662,  0.3252, -0.0501,  ..., -0.4346,  0.7034,  0.3292],\n",
      "         [-0.3497, -0.1606, -0.4492,  ..., -0.5948,  0.3007,  0.0621],\n",
      "         [ 0.0887, -0.9600,  0.1678,  ...,  0.6472,  0.1326, -0.1579]],\n",
      "\n",
      "        [[-0.4402,  0.3932, -1.4061,  ..., -0.2005,  0.6720, -0.8172],\n",
      "         [ 0.2087,  0.6763, -0.2571,  ...,  0.1412, -0.1501, -0.0122],\n",
      "         [-0.1937,  2.7545, -0.0680,  ...,  0.3875, -0.0067,  0.7153]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "logits = model(batch)\n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dd2dba",
   "metadata": {},
   "source": [
    "After completing this dummy skeleton of a GPT model, let's create the Normalization layer. The role of normalization is to stabilize training and enable faster convergance of the weights. Basically applying normalization after each attention layer, allows us to bring the mean of the outputs to 0 and the variance to 1 which allows us to keep the outputs on the same page after each layer. So reducing possible places of problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "617d8a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.3391, 0.1583, 0.0000, 0.0000, 0.4158, 0.0176],\n",
       "        [0.0000, 0.0000, 0.5672, 0.0000, 0.0000, 0.0000, 0.0000, 0.2426]],\n",
       "       grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_example = torch.randn(2,6)\n",
    "layer = nn.Sequential(nn.Linear(6,8), nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "38140da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1163],\n",
      "        [0.1012]], grad_fn=<MeanBackward1>)\n",
      "tensor([[0.0293],\n",
      "        [0.0427]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "print(mean)\n",
    "print(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0a082cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6800, -0.6800,  1.3018,  0.2451, -0.6800, -0.6800,  1.7501, -0.5771],\n",
      "        [-0.4901, -0.4901,  2.2562, -0.4901, -0.4901, -0.4901, -0.4901,  0.6843]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "tensor([[1.],\n",
      "        [1.]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "print(out_norm)\n",
    "mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "var = out_norm.var(dim=-1, keepdim=True)\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "print(mean)\n",
    "print(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "98935bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim = True, unbiased = False)\n",
    "        norm_x = (x-mean)/torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e81981",
   "metadata": {},
   "source": [
    "Some features we have used here:\n",
    "eps - When the variance of a row might be 0 then while calculating the norm, it would be dividing by zero, so we apply a very small epsilon so that it is not undefined. \n",
    "scale and shift - If during the training, the model finds that the norm_x is deviating from its expected value then the parameters of scale and shift are changed so that it brings back the mean to 0 and the var to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d77e8fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0.0000],\n",
      "        [    0.0000]], grad_fn=<MeanBackward1>)\n",
      "tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ln = LayerNorm(emb_dim=6)\n",
    "out_ln = ln(batch_example)\n",
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
    "\n",
    "print(mean)\n",
    "print(var)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dl-env)",
   "language": "python",
   "name": "dl-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
